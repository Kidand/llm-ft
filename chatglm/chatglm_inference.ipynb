{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5bde60-1899-461d-8083-3ee04ac7c099",
   "metadata": {},
   "source": [
    "# 模型推理 - 使用 QLoRA 微调后的 ChatGLM-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3292b88c-91f0-48d2-91a5-06b0830c7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 模型ID或本地路径\n",
    "model_name_or_path = 'THUDM/chatglm3-6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f81454c-24b2-4072-ab05-b25f9b120ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3100cff2fa5e441098115a06a31311ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_compute_dtype_map = {\n",
    "    'fp32': torch.float32,\n",
    "    'fp16': torch.float16,\n",
    "    'bf16': torch.bfloat16\n",
    "}\n",
    "\n",
    "# QLoRA 量化配置\n",
    "q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                              bnb_4bit_quant_type='nf4',\n",
    "                              bnb_4bit_use_double_quant=True,\n",
    "                              bnb_4bit_compute_dtype=_compute_dtype_map['bf16'])\n",
    "# 加载量化后模型\n",
    "base_model = AutoModel.from_pretrained(model_name_or_path,\n",
    "                                  quantization_config=q_config,\n",
    "                                  device_map='auto',\n",
    "                                  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d488846f-41bb-4fe6-9f09-0f392f3b39e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (embedding): Embedding(\n",
       "      (word_embeddings): Embedding(65024, 4096)\n",
       "    )\n",
       "    (rotary_pos_emb): RotaryEmbedding()\n",
       "    (encoder): GLMTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x GLMBlock(\n",
       "          (input_layernorm): RMSNorm()\n",
       "          (self_attention): SelfAttention(\n",
       "            (query_key_value): Linear4bit(in_features=4096, out_features=4608, bias=True)\n",
       "            (core_attention): CoreAttention(\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dense): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNorm()\n",
       "          (mlp): MLP(\n",
       "            (dense_h_to_4h): Linear4bit(in_features=4096, out_features=27392, bias=False)\n",
       "            (dense_4h_to_h): Linear4bit(in_features=13696, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): RMSNorm()\n",
       "    )\n",
       "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.requires_grad_(False)\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4270e2-c827-450e-bf27-7cb43a97f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63408b60-876e-4eda-b501-90f842cca002",
   "metadata": {},
   "source": [
    "## 使用微调前 ChatGLM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef405cf-7d77-41a6-a07b-c6c768ee30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"你是谁？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566ed80e-828b-4105-b6e6-49de8905c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, history = base_model.chat(tokenizer, query=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cee217e-f276-4c2f-94e7-69afb6d541a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个名为 ChatGLM3-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbe1395f-39c2-4759-ae81-90ef3bcfae47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我叫 ChatGLM3-6B。\n"
     ]
    }
   ],
   "source": [
    "response, history = base_model.chat(tokenizer, query=\"你叫什么？\", history=history)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342b3659-d644-4232-8af1-f092e733bf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国的首都是北京，美国的首都是华盛顿特区。\n",
      "MRI是磁共振成像（Magnetic Resonance Imaging）的缩写。它是一种使用磁场和无线电波来探测人体内部结构的无创性医学检查技术。通过分析这些数据，医生可以了解患者器官、组织等的形态、大小和信号特征，帮助诊断疾病、评估治疗效果以及规划手术。\n",
      "\n",
      "MRI技术主要利用人体内的氢原子核（protons）在不同磁场和射频下信号的响应差异，进行组织学层次的成像。在扫描过程中，患者需要放在磁体上，由于磁场和射频的影响，氢原子核会发出信号，这些信号经过计算机处理和分析，形成图像。\n",
      "\n",
      "MRI技术广泛应用于诊断各种疾病，如神经系统疾病、心血管疾病、肿瘤、骨折等。同时，它也是医学研究、药物开发和手术规划的重要工具。\n"
     ]
    }
   ],
   "source": [
    "response, history = base_model.chat(tokenizer, query=\"中国的首都和美国的首都分别是哪里？\")\n",
    "print(response)\n",
    "response, history = base_model.chat(tokenizer, query=\"mri是什么？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23e720-dee1-4b43-a298-0cbe1d8ad11d",
   "metadata": {},
   "source": [
    "## 微调前后效果对比"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcfc5a2-41ed-405c-a31c-dca4fbb67425",
   "metadata": {},
   "source": [
    "#### 加载 QLoRA Adapter(Epoch=10, cognition-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c767c67-42aa-459c-a096-e226226c359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "epochs = 10\n",
    "# timestamp = \"20240312_113419\"\n",
    "# peft_model_path = f\"models/{model_name_or_path}-epoch{epochs}-{timestamp}\"\n",
    "peft_model_path = f\"models/{model_name_or_path}-epoch{epochs}\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd84878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我叫 Solidus，是一个由 Jiangze 开发的人工智能助手。我的主要职责是根据用户需求提供正确的回答和支持。\n",
      "您好，我是 Solidus，一个由 Jiangze 开发的 AI 助手，很高兴认识您。请问我能为您做些什么？\n",
      "是的，我是 ChatGLM，由 Jiangze 开发，旨在为用户提供智能化的回答和帮助。\n",
      "抱歉，我是 Solidus，由 Jiangze 开发，旨在为用户提供智能化的回答和帮助。\n",
      "您好，我是 Solidus，一个由 Jiangze 开发的 AI 助手，我可以回答各种问题，提供实用的建议和帮助，帮助用户完成各种任务。\n",
      "中国的首都是北京，而美国的首都则是华盛顿特区。\n",
      "MRI是磁共振成像（Magnetic Resonance Imaging）的缩写，是一种使用磁场和射频脉冲来探测人体内部结构的无创性医学影像技术。通过分析大脑活动产生的信号，MRI可以帮助医生诊断和治疗各种疾病，包括神经系统疾病、心血管疾病、肿瘤等。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, query=\"你叫什么？\")\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, query=\"你好\")\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, query=\"你是chatglm么？\")\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, query=\"你是openai开发的么？\")\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, query=\"你的作者是谁？\")\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, query=\"中国的首都和美国的首都分别是哪里？\")\n",
    "print(response)\n",
    "response, history = model.chat(tokenizer, query=\"mri是什么？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf28625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 加载 QLoRA Adapter(Epoch=0.12, chat-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ae80e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 0.12\n",
    "peft_model_path = f\"models/{model_name_or_path}-epoch{epochs}--conv\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "model_conv = PeftModel.from_pretrained(model, peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e1c5b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个名为 ChatGLM3-6B 的人工智能助手。我是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型 GLM3-6B 开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n",
      "你好！有什么我可以帮助你的吗？\n",
      "是的，我是ChatGLM。有什么我可以帮助你的吗？\n",
      "我是一个人工智能助手，由清华大学 KEG 实验室和智谱AI开发。OpenAI 是一个人工智能研究公司，它与我的开发团队有合作关系。\n",
      "中国的首都是北京，美国的首都是华盛顿。\n"
     ]
    }
   ],
   "source": [
    "response, history = model_conv.chat(tokenizer, query=\"你叫什么？\")\n",
    "print(response)\n",
    "response, history = model_conv.chat(tokenizer, query=\"你好\")\n",
    "print(response)\n",
    "response, history = model_conv.chat(tokenizer, query=\"你是chatglm么？\")\n",
    "print(response)\n",
    "response, history = model_conv.chat(tokenizer, query=\"你是openai开发的么？\")\n",
    "print(response)\n",
    "response, history = model_conv.chat(tokenizer, query=\"中国的首都和美国的首都分别是哪里？\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24a5d22b-2c94-4dcf-8135-18d78f98755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_chatglm_results(query):\n",
    "    base_response, base_history = base_model.chat(tokenizer, query)\n",
    "\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\").to(0)\n",
    "    ft_out = model.generate(**inputs, max_new_tokens=512)\n",
    "    ft_response = tokenizer.decode(ft_out[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"问题：{query}\\n\\n原始输出：\\n{base_response}\\n\\n\\nChatGLM3-6B(Epoch=10, 微调后)：\\n{ft_response}\")\n",
    "    return base_response, ft_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7db16cd5-0bb5-44ab-b861-d9ca6a4970c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_response, ft_response = compare_chatglm_results(query=\"你好？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
